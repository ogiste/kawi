# CSM Streaming - Developer Guide

## üöÄ Quick Start for Developers

### Prerequisites
- Python 3.10+
- CUDA-compatible GPU (8GB+ VRAM recommended)
- HuggingFace account with Llama access
- 16GB+ system RAM

### Installation
```bash
git clone <this-repo>
cd csm
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Login to HuggingFace (required for Llama 3.2-1B)
huggingface-cli login
```

## üéØ Available Features

### 1. **Streaming Audio Generation**
Real-time audio generation that starts playing while still generating.

**Key Benefits:**
- ‚ö° 2-10x faster than traditional generation
- üéµ Start hearing audio immediately
- üìä Real-time performance metrics
- üîß Configurable chunk sizes and batching

### 2. **Performance Optimizations**
Multiple optimization layers for maximum speed:

- **Torch Compilation**: 2-4x speed improvement
- **CUDA Graphs**: Additional 2-5x speedup (requires CUDA)
- **Frame Batching**: Process multiple audio frames simultaneously
- **Memory Management**: Efficient GPU memory usage

### 3. **Deployment Tools**
Automated setup and deployment for various environments:

- **Vast.ai Integration**: Automated GPU instance setup
- **System Analysis**: Hardware capability checking
- **Model Caching**: Pre-download and cache models
- **Environment Setup**: Automated dependency installation

### 4. **Testing & Benchmarking**
Comprehensive testing suite with performance analysis:

- **Performance Benchmarks**: RTF (Real-Time Factor) analysis
- **Quality Testing**: Audio generation verification
- **System Compatibility**: Hardware requirement checking
- **Stress Testing**: Multiple configuration testing

## üß™ Testing Guide

### Basic Testing

#### 1. **Quick Functionality Test**
```bash
# Test basic streaming generation
python test_streaming.py --text "Hello from CSM streaming!"

# Enable real-time playback (requires sounddevice)
python test_streaming.py --play-audio
```

#### 2. **Restoration Verification**
```bash
# Verify Llama 3.2-1B integration
python test_llama_restoration.py

# Or use the automated script
./restore_to_llama.sh
```

### Advanced Testing

#### 3. **Performance Benchmarking**
```bash
# Run comprehensive benchmarks
python deploy_vast.py --benchmark

# System information and compatibility check
python deploy_vast.py --info

# Complete setup with benchmarks
python deploy_vast.py --all
```

#### 4. **Configuration Testing**
```bash
# Test different chunk sizes (smaller = more responsive)
python test_streaming.py --chunk-size 160  # Very responsive
python test_streaming.py --chunk-size 320  # Balanced (default)
python test_streaming.py --chunk-size 640  # Higher quality

# Test different batch sizes (larger = more efficient)
python test_streaming.py --batch-size 2   # Conservative
python test_streaming.py --batch-size 4   # Default
python test_streaming.py --batch-size 8   # Aggressive

# Disable optimizations for debugging
python test_streaming.py --no-compile
```

#### 5. **Custom Text Testing**
```bash
# Interactive mode (multiline input)
python test_streaming.py

# Direct text input
python test_streaming.py --text "Your custom text here"

# Save output to specific file
python test_streaming.py --text "Test" --output my_test.wav
```

## üîß Configuration Options

### StreamingConfig Parameters

```python
from streaming_generator import StreamingConfig

config = StreamingConfig(
    chunk_size_ms=320,          # Audio chunk size (80-640ms)
    frame_batch_size=4,         # Frames per batch (1-8)
    buffer_size=8,              # Playback buffer size
    enable_compilation=True,     # Torch compile optimizations
    enable_cuda_graphs=True,     # CUDA graphs (CUDA only)
    rtf_target=0.3              # Target RTF (0.3 = 3x real-time)
)
```

### Performance Tuning Guidelines

#### For Maximum Speed:
```python
config = StreamingConfig(
    chunk_size_ms=160,      # Small chunks
    frame_batch_size=8,     # Large batches
    enable_cuda_graphs=True # Maximum optimization
)
```

#### For Maximum Quality:
```python
config = StreamingConfig(
    chunk_size_ms=480,      # Larger chunks
    frame_batch_size=2,     # Smaller batches
    enable_cuda_graphs=False # Disable for stability
)
```

#### For Limited Resources:
```python
config = StreamingConfig(
    chunk_size_ms=320,      # Standard chunks
    frame_batch_size=2,     # Conservative batching
    enable_compilation=False # Disable optimizations
)
```

## üìä Performance Metrics

### Understanding RTF (Real-Time Factor)
- **RTF < 1.0**: Faster than real-time ‚úÖ
- **RTF = 1.0**: Real-time generation
- **RTF > 1.0**: Slower than real-time ‚ùå

### Target Performance:
- **Excellent**: RTF < 0.5 (2x+ faster than real-time)
- **Good**: RTF 0.5-1.0 (faster than real-time)
- **Acceptable**: RTF 1.0-2.0 (near real-time)
- **Poor**: RTF > 2.0 (needs optimization)

## üéÆ API Usage Examples

### Basic Streaming Generation
```python
from streaming_generator import load_streaming_csm_1b
import torchaudio

# Load model
generator = load_streaming_csm_1b(device="cuda")

# Generate streaming audio
audio_chunks = []
for chunk in generator.generate_streaming(
    text="Hello from CSM streaming!",
    speaker=0,
    temperature=0.9,
    topk=50
):
    audio_chunks.append(chunk)
    print(f"Generated {len(chunk)} samples")

# Save final audio
final_audio = torch.cat(audio_chunks, dim=0)
torchaudio.save("output.wav", final_audio.unsqueeze(0), generator.sample_rate)
```

### Real-time Playback
```python
# Enable real-time playback during generation
for chunk in generator.generate_streaming(
    text="This will play while generating!",
    speaker=0,
    play_audio=True  # Requires sounddevice
):
    # Audio plays automatically
    pass
```

### Batch Processing
```python
# Process multiple texts efficiently
texts = [
    "First text to generate",
    "Second text to generate", 
    "Third text to generate"
]

for i, text in enumerate(texts):
    print(f"Processing text {i+1}/{len(texts)}")
    chunks = list(generator.generate_streaming(text=text, speaker=0))
    
    # Save each generation
    audio = torch.cat(chunks, dim=0)
    torchaudio.save(f"output_{i+1}.wav", audio.unsqueeze(0), generator.sample_rate)
```

### Context-Aware Generation
```python
from generator import Segment

# Load previous audio for context
previous_audio, sr = torchaudio.load("previous_utterance.wav")
previous_audio = torchaudio.functional.resample(
    previous_audio.squeeze(0), orig_freq=sr, new_freq=generator.sample_rate
)

# Create context
context = [
    Segment(
        text="Previous utterance text",
        speaker=0,
        audio=previous_audio
    )
]

# Generate with context for voice consistency
for chunk in generator.generate_streaming(
    text="New utterance with same voice",
    speaker=0,
    context=context
):
    # Audio will match the context voice
    pass
```

## üîç Troubleshooting

### Common Issues & Solutions

#### **Slow Performance (RTF > 1.0)**
```bash
# Check CUDA availability
python -c "import torch; print('CUDA:', torch.cuda.is_available())"

# Try reducing batch size
python test_streaming.py --batch-size 2

# Disable optimizations for debugging
python test_streaming.py --no-compile
```

#### **Memory Errors**
```bash
# Reduce chunk size
python test_streaming.py --chunk-size 160

# Use smaller batch size
python test_streaming.py --batch-size 2

# Check GPU memory
nvidia-smi
```

#### **Audio Quality Issues**
```python
# Adjust generation parameters
for chunk in generator.generate_streaming(
    text="Your text",
    temperature=0.7,  # Lower = more stable
    topk=30          # Lower = more focused
):
    pass
```

#### **Authorization Issues**
```bash
# Verify HuggingFace login
huggingface-cli whoami

# Re-login if needed
huggingface-cli login
```

## üöÄ Deployment Scenarios

### Local Development
```bash
# Quick setup for development
python test_streaming.py --text "Dev test"
```

### Vast.ai GPU Instances
```bash
# Automated vast.ai setup
python deploy_vast.py --all

# Or step by step:
python deploy_vast.py --setup     # Environment setup
python deploy_vast.py --benchmark # Performance testing
```

### Production Deployment
```python
# Production-ready configuration
config = StreamingConfig(
    chunk_size_ms=320,
    frame_batch_size=4,
    enable_compilation=True,
    enable_cuda_graphs=True,
    rtf_target=0.5
)

generator = load_streaming_csm_1b(device="cuda", config=config)
```

## üìà Monitoring & Analytics

### Real-time Metrics
The system provides real-time performance monitoring:

- **Generation Speed**: RTF tracking per chunk
- **Memory Usage**: GPU memory monitoring
- **Throughput**: Audio samples per second
- **Quality Metrics**: Audio duration vs generation time

### Log Analysis
```bash
# Monitor performance during generation
python test_streaming.py --text "Long text..." | grep RTF

# Analyze benchmark results
cat benchmark_results.json | jq '.[] | {rtf, config}'
```

## üéì Advanced Features

### Custom Model Loading
```python
# Load with custom device
generator = load_streaming_csm_1b(device="cpu")  # CPU fallback
generator = load_streaming_csm_1b(device="mps")  # Apple Silicon

# Custom configuration
config = StreamingConfig(rtf_target=0.2)  # Very aggressive
generator = load_streaming_csm_1b(config=config)
```

### Integration Examples
```python
# Flask API integration
from flask import Flask, request, send_file
import io

app = Flask(__name__)
generator = load_streaming_csm_1b()

@app.route('/generate', methods=['POST'])
def generate_audio():
    text = request.json['text']
    
    chunks = list(generator.generate_streaming(text=text, speaker=0))
    audio = torch.cat(chunks, dim=0)
    
    # Return audio file
    buffer = io.BytesIO()
    torchaudio.save(buffer, audio.unsqueeze(0), generator.sample_rate, format="wav")
    buffer.seek(0)
    
    return send_file(buffer, mimetype="audio/wav")
```

## üîß Development Workflow

### 1. **Feature Development**
```bash
# Test core functionality
python test_llama_restoration.py

# Test streaming features
python test_streaming.py --no-compile  # Debug mode
```

### 2. **Performance Optimization**
```bash
# Benchmark different configurations
python deploy_vast.py --benchmark

# Test edge cases
python test_streaming.py --chunk-size 80   # Minimum
python test_streaming.py --batch-size 16   # Maximum
```

### 3. **Quality Assurance**
```bash
# Full system test
python deploy_vast.py --all

# Manual verification
python test_streaming.py --play-audio --text "Quality check"
```

### 4. **Deployment Preparation**
```bash
# Verify all dependencies
pip check

# Test deployment script
./restore_to_llama.sh

# Performance verification
python deploy_vast.py --benchmark
```

This developer guide covers all the major features and testing capabilities of the CSM streaming system. The codebase provides a comprehensive foundation for real-time speech synthesis with extensive optimization and monitoring capabilities. Created: 2025-07-13 10:26:05
Created: 2025-07-13 10:26:14
